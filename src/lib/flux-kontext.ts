import { fal } from "@fal-ai/client";
import { r2Storage } from "@/lib/services/r2-storage";

// é…ç½®FALå®¢æˆ·ç«¯
if (process.env.FAL_KEY) {
  fal.config({
    credentials: process.env.FAL_KEY
  });
  console.log(`âœ… FAL client configured with key: ${process.env.FAL_KEY.substring(0, 10)}...`);
  
  // ğŸ” éªŒè¯FALå®¢æˆ·ç«¯é…ç½®
  try {
    const keyLength = process.env.FAL_KEY.length;
    const keyPrefix = process.env.FAL_KEY.substring(0, 4);
    console.log(`ğŸ” FAL key validation:`, {
      keyLength,
      keyPrefix,
      isValidLength: keyLength > 20,
      hasValidPrefix: keyPrefix.includes('fal') || keyPrefix.includes('key')
    });
  } catch (keyError) {
    console.error('âŒ FAL key validation error:', keyError);
  }
} else {
  console.error('âŒ FAL_KEY environment variable not found');
  console.error('ğŸ” Available environment variables:', Object.keys(process.env).filter(key => key.includes('FAL')));
}

// å®šä¹‰APIç«¯ç‚¹å¸¸é‡ - ğŸ”§ æ ¹æ®FAL APIå®˜æ–¹æ–‡æ¡£å®Œå…¨ä¿®å¤ç«¯ç‚¹
export const FLUX_ENDPOINTS = {
  // ğŸ”§ Kontext å›¾åƒç¼–è¾‘ç«¯ç‚¹ï¼ˆimage-to-imageï¼‰- æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¿®å¤
  KONTEXT_PRO: "fal-ai/flux-pro/kontext",
  KONTEXT_MAX: "fal-ai/flux-pro/kontext/max", 
  
  // ğŸ”§ Kontext å¤šå›¾åƒç¼–è¾‘ç«¯ç‚¹ - æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¿®å¤
  KONTEXT_MAX_MULTI: "fal-ai/flux-pro/kontext/max/multi",
  KONTEXT_PRO_MULTI: "fal-ai/flux-pro/kontext/multi",
  
  // ğŸ”§ æ ‡å‡†FLUXæ–‡ç”Ÿå›¾ç«¯ç‚¹ - æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¿®å¤
  FLUX_PRO_TEXT_TO_IMAGE: "fal-ai/flux-pro",
  FLUX_MAX_TEXT_TO_IMAGE: "fal-ai/flux-pro/v1.1", // FLUX1.1 [pro]
  
  // ğŸ”§ æ ‡å‡†FLUXç«¯ç‚¹ - æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¿®å¤
  FLUX_SCHNELL: "fal-ai/flux/schnell",
  FLUX_DEV: "fal-ai/flux/dev",
  FLUX_GENERAL: "fal-ai/flux-general", // é€šç”¨FLUXç«¯ç‚¹ï¼Œæ”¯æŒLoRA
  FLUX_REALISM: "fal-ai/flux-lora", // ä½¿ç”¨LoRAå®ç°çœŸå®æ„Ÿ
  FLUX_ANIME: "fal-ai/flux-lora" // ä½¿ç”¨LoRAå®ç°åŠ¨æ¼«é£æ ¼
} as const;

// å®šä¹‰ç±»å‹æ¥å£
export interface FluxKontextBaseInput {
  prompt: string;
  seed?: number;
  guidance_scale?: number;
  sync_mode?: boolean;
  num_images?: number;
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  output_format?: "jpeg" | "png";
}

export interface FluxKontextImageEditInput extends FluxKontextBaseInput {
  image_url: string;
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "3:2" | "1:1" | "2:3" | "3:4" | "9:16" | "9:21";
}

export interface FluxKontextMultiImageInput extends FluxKontextBaseInput {
  image_urls: string[];
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "3:2" | "1:1" | "2:3" | "3:4" | "9:16" | "9:21";
}

export interface FluxKontextTextToImageInput extends FluxKontextBaseInput {
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "3:2" | "1:1" | "2:3" | "3:4" | "9:16" | "9:21";
}

export interface FluxKontextImage {
  url: string;
  width?: number;
  height?: number;
  content_type?: string;
}

export interface FluxKontextResult {
  images: FluxKontextImage[];
  timings?: any;
  seed?: number;
  has_nsfw_concepts?: boolean[];
  prompt?: string;
}

// Flux Kontext APIæœåŠ¡ç±»
export class FluxKontextService {
  
  /**
   * Kontext [pro] - å›¾åƒç¼–è¾‘
   * å¿«é€Ÿè¿­ä»£ç¼–è¾‘ï¼Œä¿æŒè§’è‰²ä¸€è‡´æ€§
   */
  static async editImagePro(input: FluxKontextImageEditInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ æ ¹æ®APIæ–‡æ¡£ï¼Œç§»é™¤ä¸æ”¯æŒçš„aspect_ratioå‚æ•°
      const kontextInput = {
        prompt: input.prompt,
        image_url: input.image_url,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format
        // âŒ ç§»é™¤ aspect_ratio - Kontext APIä¸æ”¯æŒæ­¤å‚æ•°
      };

      console.log(`ğŸš€ Starting editImagePro with input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        image_url: input.image_url?.substring(0, 50) + '...',
        removed_aspect_ratio: input.aspect_ratio, // è®°å½•è¢«ç§»é™¤çš„å‚æ•°
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        endpoint: FLUX_ENDPOINTS.KONTEXT_PRO,
        cleanedInput: JSON.stringify(kontextInput).substring(0, 300) + '...'
      });

      console.log(`ğŸ“¡ Calling fal.subscribe for endpoint: ${FLUX_ENDPOINTS.KONTEXT_PRO}`);
      
      const result = await fal.subscribe(FLUX_ENDPOINTS.KONTEXT_PRO, {
        input: kontextInput, // ğŸ”§ ä½¿ç”¨æ¸…ç†åçš„è¾“å…¥å‚æ•°
        logs: true,
        onQueueUpdate: (update) => {
          console.log(`ğŸ“Š Queue update:`, {
            status: update.status,
            position: (update as any).queue_position,
            logs: (update as any).logs?.map((log: any) => log.message).join(", ")
          });
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", (update as any).logs?.map((log: any) => log.message).join("\n"));
          }
        },
      });

      console.log(`ğŸ“‹ FAL subscribe result:`, {
        hasData: !!result.data,
        dataType: typeof result.data,
        hasImages: !!result.data?.images,
        imagesCount: result.data?.images?.length || 0,
        requestId: (result as any).requestId,
        fullResultKeys: result ? Object.keys(result) : [],
        dataKeys: result.data ? Object.keys(result.data) : [],
        firstImageUrl: result.data?.images?.[0]?.url?.substring(0, 50) + '...' || 'N/A',
        resultStringified: JSON.stringify(result).substring(0, 500) + '...'
      });

      if (!result.data) {
        console.error('âŒ FAL subscribe returned no data:', {
          fullResult: result,
          resultKeys: Object.keys(result),
          resultStringified: JSON.stringify(result)
        });
        throw new Error('FAL API returned no data - this may indicate a service issue or invalid request');
      }

      // ğŸ” æ£€æŸ¥dataç»“æ„
      if (!result.data.images) {
        console.error('âŒ FAL subscribe data has no images:', {
          dataKeys: Object.keys(result.data),
          dataStringified: JSON.stringify(result.data)
        });
        
        // ğŸ” å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å›¾ç‰‡å­—æ®µ
        const possibleFields = ['image', 'output', 'result'];
        for (const field of possibleFields) {
          if ((result.data as any)[field]) {
            console.log(`ğŸ” Found potential images in data.${field}:`, (result.data as any)[field]);
            if (Array.isArray((result.data as any)[field])) {
              (result.data as any).images = (result.data as any)[field];
              break;
            } else if (typeof (result.data as any)[field] === 'string') {
              (result.data as any).images = [{ url: (result.data as any)[field] }];
              break;
            }
          }
        }
        
        if (!result.data.images) {
          throw new Error('FAL API returned data without images field');
        }
      }

      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("âŒ Flux Kontext Pro editing error:", {
        error: error instanceof Error ? error.message : error,
        stack: error instanceof Error ? error.stack : undefined,
        input: {
          prompt: input.prompt?.substring(0, 100) + '...',
          image_url: input.image_url?.substring(0, 50) + '...'
        }
      });
      throw error;
    }
  }

  /**
   * Kontext [max] - å›¾åƒç¼–è¾‘
   * æœ€é«˜æ€§èƒ½ï¼Œæ”¹è¿›çš„æç¤ºéµå¾ªå’Œæ’ç‰ˆç”Ÿæˆ
   */
  static async editImageMax(input: FluxKontextImageEditInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ æ ¹æ®APIæ–‡æ¡£ï¼Œç§»é™¤ä¸æ”¯æŒçš„aspect_ratioå‚æ•°
      const kontextInput = {
        prompt: input.prompt,
        image_url: input.image_url,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format
        // âŒ ç§»é™¤ aspect_ratio - Kontext APIä¸æ”¯æŒæ­¤å‚æ•°
      };

      console.log(`ğŸš€ Starting editImageMax with input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        image_url: input.image_url?.substring(0, 50) + '...',
        removed_aspect_ratio: input.aspect_ratio, // è®°å½•è¢«ç§»é™¤çš„å‚æ•°
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        endpoint: FLUX_ENDPOINTS.KONTEXT_MAX,
        cleanedInput: JSON.stringify(kontextInput).substring(0, 300) + '...'
      });

      const result = await fal.subscribe(FLUX_ENDPOINTS.KONTEXT_MAX, {
        input: kontextInput, // ğŸ”§ ä½¿ç”¨æ¸…ç†åçš„è¾“å…¥å‚æ•°
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", update.logs?.map(log => log.message).join("\n"));
          }
        },
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("âŒ Flux Kontext Max editing error:", {
        error: error instanceof Error ? error.message : error,
        stack: error instanceof Error ? error.stack : undefined,
        input: {
          prompt: input.prompt?.substring(0, 100) + '...',
          image_url: input.image_url?.substring(0, 50) + '...',
          removed_aspect_ratio: input.aspect_ratio
        }
      });
      throw error;
    }
  }

  /**
   * Kontext [max] - å¤šå›¾åƒç¼–è¾‘ï¼ˆå®éªŒæ€§ï¼‰
   * å¤„ç†å¤šä¸ªå›¾åƒçš„ç¼–è¾‘åŠŸèƒ½
   */
  static async editMultiImageMax(input: FluxKontextMultiImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ æ ¹æ®APIæ–‡æ¡£ï¼Œç§»é™¤ä¸æ”¯æŒçš„aspect_ratioå‚æ•°
      const kontextInput = {
        prompt: input.prompt,
        image_urls: input.image_urls,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format
        // âŒ ç§»é™¤ aspect_ratio - Kontext APIä¸æ”¯æŒæ­¤å‚æ•°
      };

      console.log(`ğŸš€ Starting editMultiImageMax with input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        image_urls_count: input.image_urls?.length || 0,
        removed_aspect_ratio: input.aspect_ratio, // è®°å½•è¢«ç§»é™¤çš„å‚æ•°
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        endpoint: FLUX_ENDPOINTS.KONTEXT_MAX_MULTI,
        cleanedInput: JSON.stringify(kontextInput).substring(0, 300) + '...'
      });

      const result = await fal.subscribe(FLUX_ENDPOINTS.KONTEXT_MAX_MULTI, {
        input: kontextInput, // ğŸ”§ ä½¿ç”¨æ¸…ç†åçš„è¾“å…¥å‚æ•°
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", update.logs?.map(log => log.message).join("\n"));
          }
        },
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("âŒ Flux Kontext Max multi-image editing error:", {
        error: error instanceof Error ? error.message : error,
        stack: error instanceof Error ? error.stack : undefined,
        input: {
          prompt: input.prompt?.substring(0, 100) + '...',
          image_urls_count: input.image_urls?.length || 0,
          removed_aspect_ratio: input.aspect_ratio
        }
      });
      throw error;
    }
  }

  /**
   * Kontext [max] - æ–‡æœ¬ç”Ÿæˆå›¾åƒ
   * æœ€é«˜æ€§èƒ½çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
   */
  static async textToImageMax(input: FluxKontextTextToImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ è½¬æ¢å‚æ•°æ ¼å¼ï¼šæ ‡å‡†FLUXç«¯ç‚¹ä½¿ç”¨image_sizeè€Œä¸æ˜¯aspect_ratio
      const fluxInput = {
        prompt: input.prompt,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        sync_mode: input.sync_mode,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        // ğŸ”§ å°†aspect_ratioè½¬æ¢ä¸ºimage_sizeæ ¼å¼
        image_size: this.convertAspectRatioToImageSize(input.aspect_ratio)
      };

      console.log(`ğŸš€ Starting textToImageMax with converted input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        original_aspect_ratio: input.aspect_ratio,
        converted_image_size: fluxInput.image_size,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        endpoint: FLUX_ENDPOINTS.FLUX_MAX_TEXT_TO_IMAGE
      });

      console.log(`ğŸ“¡ Calling fal.subscribe for endpoint: ${FLUX_ENDPOINTS.FLUX_MAX_TEXT_TO_IMAGE}`);
      
      const result = await fal.subscribe(FLUX_ENDPOINTS.FLUX_MAX_TEXT_TO_IMAGE, {
        input: fluxInput,
        logs: true,
        onQueueUpdate: (update) => {
          console.log(`ğŸ“Š Queue update:`, {
            status: update.status,
            position: (update as any).queue_position,
            logs: (update as any).logs?.map((log: any) => log.message).join(", ")
          });
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", (update as any).logs?.map((log: any) => log.message).join("\n"));
          }
        },
      });

      console.log(`ğŸ“‹ FAL subscribe result:`, {
        hasData: !!result.data,
        dataType: typeof result.data,
        hasImages: !!result.data?.images,
        imagesCount: result.data?.images?.length || 0,
        requestId: (result as any).requestId,
        fullResultKeys: result ? Object.keys(result) : [],
        dataKeys: result.data ? Object.keys(result.data) : [],
        firstImageUrl: result.data?.images?.[0]?.url?.substring(0, 50) + '...' || 'N/A',
        resultStringified: JSON.stringify(result).substring(0, 500) + '...'
      });

      if (!result.data) {
        console.error('âŒ FAL subscribe returned no data:', {
          fullResult: result,
          resultKeys: Object.keys(result),
          resultStringified: JSON.stringify(result)
        });
        throw new Error('FAL API returned no data - this may indicate a service issue or invalid request');
      }

      // ğŸ” æ£€æŸ¥dataç»“æ„
      if (!result.data.images) {
        console.error('âŒ FAL subscribe data has no images:', {
          dataKeys: Object.keys(result.data),
          dataStringified: JSON.stringify(result.data)
        });
        
        // ğŸ” å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å›¾ç‰‡å­—æ®µ
        const possibleFields = ['image', 'output', 'result'];
        for (const field of possibleFields) {
          if ((result.data as any)[field]) {
            console.log(`ğŸ” Found potential images in data.${field}:`, (result.data as any)[field]);
            if (Array.isArray((result.data as any)[field])) {
              (result.data as any).images = (result.data as any)[field];
              break;
            } else if (typeof (result.data as any)[field] === 'string') {
              (result.data as any).images = [{ url: (result.data as any)[field] }];
              break;
            }
          }
        }
        
        if (!result.data.images) {
          throw new Error('FAL API returned data without images field');
        }
      }

      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("âŒ Flux Max text-to-image error:", {
        error: error instanceof Error ? error.message : error,
        stack: error instanceof Error ? error.stack : undefined,
        input: {
          prompt: input.prompt?.substring(0, 100) + '...',
          aspect_ratio: input.aspect_ratio
        }
      });
      throw error;
    }
  }

  /**
   * Kontext [pro] - å¤šå›¾åƒç¼–è¾‘ï¼ˆå®éªŒæ€§ï¼‰
   * Proç‰ˆæœ¬çš„å¤šå›¾åƒå¤„ç†åŠŸèƒ½
   */
  static async editMultiImagePro(input: FluxKontextMultiImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ æ ¹æ®APIæ–‡æ¡£ï¼Œç§»é™¤ä¸æ”¯æŒçš„aspect_ratioå‚æ•°
      const kontextInput = {
        prompt: input.prompt,
        image_urls: input.image_urls,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format
        // âŒ ç§»é™¤ aspect_ratio - Kontext APIä¸æ”¯æŒæ­¤å‚æ•°
      };

      console.log(`ğŸš€ Starting editMultiImagePro with input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        image_urls_count: input.image_urls?.length || 0,
        removed_aspect_ratio: input.aspect_ratio, // è®°å½•è¢«ç§»é™¤çš„å‚æ•°
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        endpoint: FLUX_ENDPOINTS.KONTEXT_PRO_MULTI,
        cleanedInput: JSON.stringify(kontextInput).substring(0, 300) + '...'
      });

      const result = await fal.subscribe(FLUX_ENDPOINTS.KONTEXT_PRO_MULTI, {
        input: kontextInput, // ğŸ”§ ä½¿ç”¨æ¸…ç†åçš„è¾“å…¥å‚æ•°
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", update.logs?.map(log => log.message).join("\n"));
          }
        },
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("âŒ Flux Kontext Pro multi-image editing error:", {
        error: error instanceof Error ? error.message : error,
        stack: error instanceof Error ? error.stack : undefined,
        input: {
          prompt: input.prompt?.substring(0, 100) + '...',
          image_urls_count: input.image_urls?.length || 0,
          removed_aspect_ratio: input.aspect_ratio
        }
      });
      throw error;
    }
  }

  /**
   * Kontext [pro] - æ–‡æœ¬ç”Ÿæˆå›¾åƒ
   * Proç‰ˆæœ¬çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
   */
  static async textToImagePro(input: FluxKontextTextToImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ è½¬æ¢å‚æ•°æ ¼å¼ï¼šæ ‡å‡†FLUXç«¯ç‚¹ä½¿ç”¨image_sizeè€Œä¸æ˜¯aspect_ratio
      const fluxInput = {
        prompt: input.prompt,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        sync_mode: input.sync_mode,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        // ğŸ”§ å°†aspect_ratioè½¬æ¢ä¸ºimage_sizeæ ¼å¼
        image_size: this.convertAspectRatioToImageSize(input.aspect_ratio)
      };

      console.log(`ğŸš€ Starting textToImagePro with converted input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        original_aspect_ratio: input.aspect_ratio,
        converted_image_size: fluxInput.image_size,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        endpoint: FLUX_ENDPOINTS.FLUX_PRO_TEXT_TO_IMAGE
      });

      console.log(`ğŸ“¡ Calling fal.subscribe for endpoint: ${FLUX_ENDPOINTS.FLUX_PRO_TEXT_TO_IMAGE}`);
      
      const result = await fal.subscribe(FLUX_ENDPOINTS.FLUX_PRO_TEXT_TO_IMAGE, {
        input: fluxInput,
        logs: true,
        onQueueUpdate: (update) => {
          console.log(`ğŸ“Š Queue update:`, {
            status: update.status,
            position: (update as any).queue_position,
            logs: (update as any).logs?.map((log: any) => log.message).join(", ")
          });
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", (update as any).logs?.map((log: any) => log.message).join("\n"));
          }
        },
      });

      console.log(`ğŸ“‹ FAL subscribe result:`, {
        hasData: !!result.data,
        dataType: typeof result.data,
        hasImages: !!result.data?.images,
        imagesCount: result.data?.images?.length || 0,
        requestId: (result as any).requestId,
        fullResultKeys: result ? Object.keys(result) : [],
        dataKeys: result.data ? Object.keys(result.data) : [],
        firstImageUrl: result.data?.images?.[0]?.url?.substring(0, 50) + '...' || 'N/A',
        resultStringified: JSON.stringify(result).substring(0, 500) + '...'
      });

      if (!result.data) {
        console.error('âŒ FAL subscribe returned no data:', {
          fullResult: result,
          resultKeys: Object.keys(result),
          resultStringified: JSON.stringify(result)
        });
        throw new Error('FAL API returned no data - this may indicate a service issue or invalid request');
      }

      // ğŸ” æ£€æŸ¥dataç»“æ„
      if (!result.data.images) {
        console.error('âŒ FAL subscribe data has no images:', {
          dataKeys: Object.keys(result.data),
          dataStringified: JSON.stringify(result.data)
        });
        
        // ğŸ” å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å›¾ç‰‡å­—æ®µ
        const possibleFields = ['image', 'output', 'result'];
        for (const field of possibleFields) {
          if ((result.data as any)[field]) {
            console.log(`ğŸ” Found potential images in data.${field}:`, (result.data as any)[field]);
            if (Array.isArray((result.data as any)[field])) {
              (result.data as any).images = (result.data as any)[field];
              break;
            } else if (typeof (result.data as any)[field] === 'string') {
              (result.data as any).images = [{ url: (result.data as any)[field] }];
              break;
            }
          }
        }
        
        if (!result.data.images) {
          throw new Error('FAL API returned data without images field');
        }
      }

      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("âŒ Flux Pro text-to-image error:", {
        error: error instanceof Error ? error.message : error,
        stack: error instanceof Error ? error.stack : undefined,
        input: {
          prompt: input.prompt?.substring(0, 100) + '...',
          aspect_ratio: input.aspect_ratio
        }
      });
      throw error;
    }
  }

  /**
   * ä¸Šä¼ æ–‡ä»¶åˆ°å­˜å‚¨æœåŠ¡
   * ğŸ”§ åŒæ—¶ä¸Šä¼ åˆ°FALå’ŒR2å­˜å‚¨ï¼Œä¼˜å…ˆä½¿ç”¨FALå­˜å‚¨é“¾æ¥
   */
  static async uploadFile(file: File): Promise<string> {
    try {
      console.log("ğŸ“¤ Starting dual storage upload:", file.name);
      
      // ğŸ”§ ä¼˜å…ˆä¸Šä¼ åˆ°FALå­˜å‚¨ï¼ˆç¡®ä¿APIå…¼å®¹æ€§ï¼‰
      let falUrl: string | null = null;
      let r2Url: string | null = null;
      
      try {
        console.log("ğŸ“¤ Uploading to FAL storage (primary):", file.name);
        falUrl = await fal.storage.upload(file);
        console.log("âœ… FAL upload successful:", falUrl);
      } catch (falError) {
        console.error("âŒ FAL upload failed:", falError);
      }
      
      // ğŸ”§ åŒæ—¶å°è¯•ä¸Šä¼ åˆ°R2å­˜å‚¨ï¼ˆå¤‡ä»½å’Œç”¨æˆ·æŸ¥çœ‹ï¼‰
      const isR2Enabled = process.env.NEXT_PUBLIC_ENABLE_R2 === "true";
      const hasR2Config = process.env.R2_ACCOUNT_ID && 
                         process.env.R2_ACCESS_KEY_ID && 
                         process.env.R2_SECRET_ACCESS_KEY &&
                         process.env.R2_BUCKET_NAME;

      if (isR2Enabled && hasR2Config) {
        try {
          console.log("ğŸ“¤ Uploading to R2 storage (backup):", file.name);
          r2Url = await r2Storage.uploadFile(file);
          console.log("âœ… R2 upload successful:", r2Url);
        } catch (r2Error) {
          console.warn("âš ï¸ R2 upload failed (non-critical):", r2Error);
        }
      } else {
        console.log("â„¹ï¸ R2 storage not configured, skipping R2 upload");
      }
      
      // ğŸ”§ ä¼˜å…ˆè¿”å›FAL URLï¼Œå¦‚æœFALå¤±è´¥åˆ™è¿”å›R2 URL
      if (falUrl) {
        console.log("ğŸ¯ Using FAL URL as primary:", falUrl);
        if (r2Url) {
          console.log("ğŸ“‹ R2 URL available as backup:", r2Url);
        }
        return falUrl;
      } else if (r2Url) {
        console.log("ğŸ¯ FAL failed, using R2 URL as fallback:", r2Url);
        return r2Url;
      } else {
        throw new Error("Both FAL and R2 storage uploads failed");
      }
      
    } catch (error) {
      console.error("âŒ Dual storage upload failed:", error);
      throw error;
    }
  }

  /**
   * å°†AIç”Ÿæˆçš„å›¾ç‰‡ä¿å­˜åˆ°R2å­˜å‚¨
   * @param imageUrl AIç”Ÿæˆçš„å›¾ç‰‡URL
   * @param prompt ç”Ÿæˆæç¤ºè¯
   */
  static async saveGeneratedImageToR2(imageUrl: string, prompt: string): Promise<string> {
    try {
      // æ£€æŸ¥æ˜¯å¦å¯ç”¨R2å­˜å‚¨
      const isR2Enabled = process.env.NEXT_PUBLIC_ENABLE_R2 === "true";
      const hasR2Config = process.env.R2_ACCOUNT_ID && 
                         process.env.R2_ACCESS_KEY_ID && 
                         process.env.R2_SECRET_ACCESS_KEY &&
                         process.env.R2_BUCKET_NAME;

      if (!isR2Enabled || !hasR2Config) {
        console.log("â„¹ï¸ R2 storage not configured, returning original URL");
        return imageUrl; // å¦‚æœR2æœªé…ç½®ï¼Œè¿”å›åŸå§‹URL
      }

      console.log("ğŸ“¤ Saving AI generated image to R2:", imageUrl);
      
      // ä½¿ç”¨R2å­˜å‚¨çš„uploadFromUrlæ–¹æ³•
      const result = await r2Storage.uploadFromUrl(imageUrl, prompt);
      
      console.log("âœ… AI generated image saved to R2 successfully:", result);
      return result;
      
    } catch (error) {
      console.error("âŒ Failed to save AI generated image to R2:", error);
      // å¦‚æœä¿å­˜å¤±è´¥ï¼Œè¿”å›åŸå§‹URL
      return imageUrl;
    }
  }

  /**
   * é˜Ÿåˆ—æäº¤ï¼ˆç”¨äºé•¿æ—¶é—´è¿è¡Œçš„è¯·æ±‚ï¼‰
   */
  static async submitToQueue(endpoint: string, input: any): Promise<{ request_id: string }> {
    try {
      const result = await fal.queue.submit(endpoint, {
        input,
        webhookUrl: process.env.NEXT_PUBLIC_SITE_URL + "/api/webhooks/fal"
      });
      return result;
    } catch (error) {
      console.error("Queue submission error:", error);
      throw error;
    }
  }

  /**
   * æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
   */
  static async checkQueueStatus(endpoint: string, requestId: string): Promise<any> {
    try {
      const status = await fal.queue.status(endpoint, {
        requestId,
        logs: true,
      });
      return status;
    } catch (error) {
      console.error("Queue status check error:", error);
      throw error;
    }
  }

  /**
   * è·å–é˜Ÿåˆ—ç»“æœ
   */
  static async getQueueResult(endpoint: string, requestId: string): Promise<FluxKontextResult> {
    try {
      const result = await fal.queue.result(endpoint, {
        requestId
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("Queue result retrieval error:", error);
      throw error;
    }
  }

  /**
   * FLUX.1 [schnell] - æ–‡æœ¬ç”Ÿæˆå›¾åƒ
   * è¶…å¿«é€Ÿç”Ÿæˆï¼Œ1-4æ­¥å®Œæˆ
   */
  static async textToImageSchnell(input: FluxKontextTextToImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ è½¬æ¢å‚æ•°æ ¼å¼ï¼šFLUX Schnellç«¯ç‚¹ä½¿ç”¨image_sizeè€Œä¸æ˜¯aspect_ratio
      const schnellInput = {
        prompt: input.prompt,
        seed: input.seed,
        sync_mode: input.sync_mode,
        num_images: input.num_images,
        output_format: input.output_format,
        // ğŸ”§ å°†aspect_ratioè½¬æ¢ä¸ºimage_sizeæ ¼å¼
        image_size: this.convertAspectRatioToImageSize(input.aspect_ratio),
        // ğŸ”§ Schnellæ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ¨ç†æ­¥éª¤
        num_inference_steps: 4
      };

      console.log(`ğŸš€ Starting textToImageSchnell with converted input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        original_aspect_ratio: input.aspect_ratio,
        converted_image_size: schnellInput.image_size,
        endpoint: FLUX_ENDPOINTS.FLUX_SCHNELL
      });

      const result = await fal.subscribe(FLUX_ENDPOINTS.FLUX_SCHNELL, {
        input: schnellInput,
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", update.logs?.map(log => log.message).join("\n"));
          }
        },
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("Flux Schnell text-to-image error:", error);
      throw error;
    }
  }

  /**
   * FLUX.1 [dev] - æ–‡æœ¬ç”Ÿæˆå›¾åƒ
   * å¼€å‘æ¨¡å‹ï¼Œå¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
   */
  static async textToImageDev(input: FluxKontextTextToImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ è½¬æ¢å‚æ•°æ ¼å¼ï¼šFLUX Generalç«¯ç‚¹ä½¿ç”¨image_sizeè€Œä¸æ˜¯aspect_ratio
      const fluxInput = {
        prompt: input.prompt,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        sync_mode: input.sync_mode,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        // ğŸ”§ å°†aspect_ratioè½¬æ¢ä¸ºimage_sizeæ ¼å¼
        image_size: this.convertAspectRatioToImageSize(input.aspect_ratio)
      };

      console.log(`ğŸš€ Starting textToImageDev with converted input:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        original_aspect_ratio: input.aspect_ratio,
        converted_image_size: fluxInput.image_size,
        endpoint: FLUX_ENDPOINTS.FLUX_GENERAL
      });

      const result = await fal.subscribe(FLUX_ENDPOINTS.FLUX_GENERAL, {
        input: fluxInput,
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", update.logs?.map(log => log.message).join("\n"));
          }
        },
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("Flux Dev text-to-image error:", error);
      throw error;
    }
  }

  /**
   * FLUX Realism - æ–‡æœ¬ç”Ÿæˆå›¾åƒ
   * ç…§ç‰‡çº§çœŸå®æ„Ÿå›¾åƒç”Ÿæˆ
   */
  static async textToImageRealism(input: FluxKontextTextToImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ ä½¿ç”¨FLUX Generalç«¯ç‚¹å’ŒLoRAå®ç°çœŸå®æ„Ÿé£æ ¼
      const realismInput = {
        prompt: input.prompt,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        sync_mode: input.sync_mode,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        // ğŸ”§ å°†aspect_ratioè½¬æ¢ä¸ºimage_sizeæ ¼å¼
        image_size: this.convertAspectRatioToImageSize(input.aspect_ratio),
        // ğŸ”§ ä½¿ç”¨LoRAå®ç°çœŸå®æ„Ÿé£æ ¼
        loras: [
          {
            path: "https://huggingface.co/XLabs-AI/flux-RealismLora/resolve/main/lora.safetensors",
            scale: 0.8
          }
        ]
      };

      console.log(`ğŸš€ Starting textToImageRealism with LoRA:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        endpoint: FLUX_ENDPOINTS.FLUX_GENERAL,
        loras: realismInput.loras
      });
      
      const result = await fal.subscribe(FLUX_ENDPOINTS.FLUX_GENERAL, {
        input: realismInput,
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", update.logs?.map(log => log.message).join("\n"));
          }
        },
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("Flux Realism text-to-image error:", error);
      throw error;
    }
  }

  /**
   * FLUX Anime - æ–‡æœ¬ç”Ÿæˆå›¾åƒ
   * åŠ¨æ¼«é£æ ¼å›¾åƒç”Ÿæˆ
   */
  static async textToImageAnime(input: FluxKontextTextToImageInput): Promise<FluxKontextResult> {
    try {
      // ğŸ”§ ä½¿ç”¨FLUX Generalç«¯ç‚¹å’ŒLoRAå®ç°åŠ¨æ¼«é£æ ¼
      const animeInput = {
        prompt: input.prompt,
        seed: input.seed,
        guidance_scale: input.guidance_scale,
        sync_mode: input.sync_mode,
        num_images: input.num_images,
        safety_tolerance: input.safety_tolerance,
        output_format: input.output_format,
        // ğŸ”§ å°†aspect_ratioè½¬æ¢ä¸ºimage_sizeæ ¼å¼
        image_size: this.convertAspectRatioToImageSize(input.aspect_ratio),
        // ğŸ”§ ä½¿ç”¨LoRAå®ç°åŠ¨æ¼«é£æ ¼
        loras: [
          {
            path: "https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-AnimeStyle/resolve/main/FLUX-dev-lora-AnimeStyle.safetensors",
            scale: 0.9
          }
        ]
      };

      console.log(`ğŸš€ Starting textToImageAnime with LoRA:`, {
        prompt: input.prompt?.substring(0, 100) + '...',
        endpoint: FLUX_ENDPOINTS.FLUX_GENERAL,
        loras: animeInput.loras
      });
      
      const result = await fal.subscribe(FLUX_ENDPOINTS.FLUX_GENERAL, {
        input: animeInput,
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            console.log("Generation progress:", update.logs?.map(log => log.message).join("\n"));
          }
        },
      });
      return result.data as FluxKontextResult;
    } catch (error) {
      console.error("Flux Anime text-to-image error:", error);
      throw error;
    }
  }

  /**
   * å°†aspect_ratioè½¬æ¢ä¸ºimage_sizeæ ¼å¼
   */
  static convertAspectRatioToImageSize(aspect_ratio?: string): "square_hd" | "square" | "portrait_4_3" | "portrait_16_9" | "landscape_4_3" | "landscape_16_9" | undefined {
    if (!aspect_ratio) return "landscape_4_3"; // é»˜è®¤å€¼

    // å°†aspect_ratioè½¬æ¢ä¸ºFAL APIæ”¯æŒçš„image_sizeæšä¸¾
    switch (aspect_ratio) {
      case "1:1":
        return "square_hd";
      case "4:3":
        return "landscape_4_3";
      case "3:4":
        return "portrait_4_3";
      case "16:9":
        return "landscape_16_9";
      case "9:16":
        return "portrait_16_9";
      case "21:9":
        return "landscape_16_9"; // 21:9æ˜ å°„åˆ°16:9ï¼Œå› ä¸ºFALä¸æ”¯æŒ21:9
      case "9:21":
        return "portrait_16_9"; // 9:21æ˜ å°„åˆ°9:16ï¼Œå› ä¸ºFALä¸æ”¯æŒ9:21
      case "3:2":
        return "landscape_4_3"; // 3:2æ˜ å°„åˆ°4:3
      case "2:3":
        return "portrait_4_3"; // 2:3æ˜ å°„åˆ°3:4
      default:
        return "landscape_4_3"; // é»˜è®¤æ¨ªå‘4:3
    }
  }
} 